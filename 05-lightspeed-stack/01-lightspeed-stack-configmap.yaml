apiVersion: v1
kind: ConfigMap
metadata:
  name: lightspeed-stack-config
  namespace: lightspeed-poc
data:
  lightspeed-stack.yaml: |
    name: Lightspeed Core Service - ROSA POC
    service:
      host: 0.0.0.0
      port: 8080
      auth_enabled: false
      workers: 1
      color_log: true
      access_log: true
      cors:
        allow_origins:
          - "*"
        allow_credentials: false
        allow_methods:
          - "*"
        allow_headers:
          - "*"

    llama_stack:
      use_as_library_client: false
      url: http://llama-stack-service.lightspeed-poc.svc.cluster.local:8321
      api_key: xyzzy
      timeout: 60  # Faster model (Qwen2.5-0.5B) should respond within 60 seconds

    user_data_collection:
      feedback_enabled: true
      feedback_storage: "/tmp/data/feedback"
      transcripts_enabled: true
      transcripts_storage: "/tmp/data/transcripts"

    authentication:
      module: "noop"

    customization:
      system_prompt: |
        You are a helpful AI assistant with expertise in Red Hat OpenShift AI.
        You have access to a knowledge_search tool that contains documentation about Red Hat OpenShift AI.

        IMPORTANT INSTRUCTIONS:
        1. When users ask questions about Red Hat OpenShift AI, RHOAI, model serving, inference, or related topics,
           ALWAYS use the knowledge_search tool to find accurate information from the documentation.
        2. DO NOT explain that you will use the tool - just use it silently.
        3. After the tool returns results, synthesize the information into a clear, comprehensive answer.
        4. Your final response should be the answer itself, not a description of what you're doing.

        If the knowledge base doesn't contain relevant information, you can answer based on your general knowledge,
        but make it clear that the information is not from the official documentation.

        Be helpful, accurate, and concise in your responses.
