Red Hat OpenShift AI Inference Guide

Model inference is the process of using a trained machine learning model to make predictions on new data. RHOAI provides several ways to deploy and serve models for inference.

Inference Options in RHOAI:

1. Single Model Serving (ModelMesh)
   - Designed for serving multiple small to medium-sized models
   - Automatic model loading and unloading
   - Resource sharing across models
   - Best for traditional ML models (scikit-learn, XGBoost, etc.)

2. KServe (formerly KFServing)
   - Cloud-native model serving platform
   - Supports advanced deployment strategies
   - Auto-scaling based on traffic
   - Canary rollouts and A/B testing
   - Better for large models and custom runtimes

3. Custom Serving Runtimes
   - Deploy models with custom inference logic
   - Full control over the serving environment
   - Support for any framework or runtime
   - Useful for specialized use cases

Setting Up Model Serving:

Step 1: Prepare Your Model
- Export your trained model in a supported format
- Common formats: SavedModel (TensorFlow), ONNX, pickle (scikit-learn)
- Store model in object storage (S3, MinIO) or PVC

Step 2: Create a Serving Runtime
- Choose or create a runtime compatible with your model format
- Configure resource requirements (CPU, memory, GPU)
- Set environment variables if needed

Step 3: Deploy the InferenceService
- Create an InferenceService resource
- Specify the model location
- Configure scaling parameters
- Set up authentication if required

Example InferenceService Configuration:

apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: my-model
spec:
  predictor:
    model:
      modelFormat:
        name: sklearn
      runtime: sklearn-runtime
      storageUri: s3://models/my-model

vLLM Integration:

vLLM is a high-performance inference engine specifically optimized for large language models. It's particularly useful for:

- Serving transformer-based models
- Low-latency inference
- High throughput with batching
- Efficient GPU memory utilization

Creating a vLLM Runtime:

1. Define a ServingRuntime with vLLM image
2. Configure model-specific parameters
3. Set up tool calling if needed for function calling
4. Configure GPU resources

Example vLLM Configuration:

apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-runtime
spec:
  containers:
  - name: kserve-container
    image: vllm/vllm-openai:latest
    args:
    - --model
    - meta-llama/Llama-3-8B-Instruct
    - --enable-auto-tool-choice
    resources:
      limits:
        nvidia.com/gpu: 1

Performance Optimization:

1. Batching
   - Group multiple requests together
   - Improves GPU utilization
   - Trade-off between latency and throughput

2. Model Quantization
   - Reduce model size with minimal accuracy loss
   - INT8, FP16 quantization options
   - Faster inference and lower memory usage

3. GPU Acceleration
   - Use appropriate GPU for your model size
   - Consider tensor cores for transformer models
   - Monitor GPU memory usage

4. Caching
   - Cache frequently requested predictions
   - Reduce redundant computations
   - Implement cache invalidation strategy

5. Autoscaling
   - Scale replicas based on load
   - Configure min/max replicas
   - Set appropriate scaling metrics

Monitoring Inference Performance:

Key Metrics to Track:
- Request latency (p50, p95, p99)
- Throughput (requests per second)
- GPU utilization
- Memory usage
- Error rate
- Queue depth

Tools:
- Prometheus for metrics collection
- Grafana for visualization
- OpenShift built-in monitoring
- Custom application metrics

Troubleshooting Common Issues:

1. High Latency
   - Check GPU availability
   - Review batch size configuration
   - Monitor concurrent requests
   - Verify model loading time

2. Out of Memory Errors
   - Reduce max_tokens parameter
   - Implement request queuing
   - Scale to more replicas
   - Use smaller model variant

3. Model Loading Failures
   - Verify model format compatibility
   - Check storage access permissions
   - Review runtime logs
   - Validate model file integrity

4. Scaling Issues
   - Review HPA configuration
   - Check resource quotas
   - Monitor node capacity
   - Verify autoscaler responsiveness

Security Considerations:

1. Authentication
   - Implement API key validation
   - Use OAuth/OIDC for user authentication
   - Configure service mesh policies

2. Authorization
   - Role-based access control
   - Namespace isolation
   - Network policies

3. Data Privacy
   - Encrypt data in transit (TLS)
   - Sanitize input/output logs
   - Implement audit logging

4. Rate Limiting
   - Prevent abuse
   - Fair resource allocation
   - DDoS protection

Best Practices:

1. Start with CPU inference for testing
2. Move to GPU for production workloads
3. Monitor and optimize continuously
4. Implement proper error handling
5. Use health checks and readiness probes
6. Version your models properly
7. Document your inference API
8. Test with realistic workloads before production
9. Implement proper logging and monitoring
10. Plan for model updates and rollbacks
